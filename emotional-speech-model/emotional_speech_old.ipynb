{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XPKZMTSSSivj",
    "outputId": "abdd1516-f6ca-4b12-d7b0-444bbfa080f9"
   },
   "outputs": [],
   "source": [
    "# Download dataset. This is not needed if you have the dataset already\n",
    "\n",
    "# import kagglehub\n",
    "\n",
    "# path = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Reformatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SnlbjELrSivk",
    "outputId": "798e7ac8-364b-4ba1-caa3-c1601f3584bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling rate: 48000 Hz\n"
     ]
    }
   ],
   "source": [
    "# Function to check the sampling rate of a wav file and valid file path\n",
    "\n",
    "import wave\n",
    "import contextlib\n",
    "\n",
    "def check_sampling_rate(file_path):\n",
    "    try:\n",
    "        with contextlib.closing(wave.open(file_path, 'r')) as wav_file:\n",
    "            sample_rate = wav_file.getframerate()\n",
    "            print(f\"Sampling rate: {sample_rate} Hz\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "file_path = 'data/ravdess-emotional-speech-audio/versions/1/Actor_01/03-01-01-01-01-01-01.wav'\n",
    "check_sampling_rate(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1l5AM4UkSivk",
    "outputId": "dba92514-0d9b-4bfc-c309-37cc31b2846f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        emotion                          file_path\n",
      "0       disgust  Actor_24_03-01-07-01-02-01-24.wav\n",
      "1     surprised  Actor_24_03-01-08-02-01-01-24.wav\n",
      "2           sad  Actor_24_03-01-04-01-02-01-24.wav\n",
      "3         angry  Actor_24_03-01-05-01-01-01-24.wav\n",
      "4           sad  Actor_24_03-01-04-02-01-01-24.wav\n",
      "...         ...                                ...\n",
      "1435    neutral  Actor_07_03-01-01-01-01-01-07.wav\n",
      "1436      happy  Actor_07_03-01-03-02-01-02-07.wav\n",
      "1437    neutral  Actor_07_03-01-01-01-01-02-07.wav\n",
      "1438    disgust  Actor_07_03-01-07-02-01-02-07.wav\n",
      "1439       calm  Actor_07_03-01-02-02-01-01-07.wav\n",
      "\n",
      "[1440 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "File naming convention\n",
    "\n",
    "Each of the 1440 files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 03-01-06-01-02-01-12.wav). These identifiers define the stimulus characteristics:\n",
    "\n",
    "Filename identifiers\n",
    "\n",
    "Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "\n",
    "Vocal channel (01 = speech, 02 = song).\n",
    "\n",
    "Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "\n",
    "Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    "\n",
    "Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "\n",
    "Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "emotion_mapping = {\n",
    "    \"01\": \"neutral\",\n",
    "    \"02\": \"calm\",\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"06\": \"fearful\",\n",
    "    \"07\": \"disgust\",\n",
    "    \"08\": \"surprised\"\n",
    "}\n",
    "\n",
    "file_dir = \"data/ravdess-emotional-speech-audio/versions/1/\"\n",
    "\n",
    "data = []\n",
    "\n",
    "for actor in os.listdir(file_dir):\n",
    "    actor_path = os.path.join(file_dir, actor)\n",
    "    \n",
    "    if os.path.isdir(actor_path) and actor.startswith(\"Actor_\"):\n",
    "        actor_number = actor.split(\"_\")[-1]\n",
    "\n",
    "        for file in os.listdir(actor_path):\n",
    "            if file.endswith(\".wav\"):\n",
    "                emotion_code = file[6:8]\n",
    "                emotion = emotion_mapping.get(emotion_code, \"unknown\")\n",
    "                formatted_filename = f\"Actor_{actor_number}_{file}\"\n",
    "                data.append({\"emotion\": emotion, \"file_path\": formatted_filename})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove calm emotion\n",
    "df = df[df.emotion != 'calm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting waveforms and spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vQvjb4mASivk"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "from matplotlib.patches import Rectangle\n",
    "from torchaudio.utils import download_asset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.random.manual_seed(42)\n",
    "\n",
    "def plot_waveform(waveform, sr, title=None, ax=None):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sr\n",
    "\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(num_channels, 1)\n",
    "    ax.plot(time_axis, waveform[0], linewidth=1)\n",
    "    ax.set_xlim([0, time_axis[-1]])\n",
    "    ax.set_title(title)\n",
    "\n",
    "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\", ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    ax.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\", interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_wave_and_spec():\n",
    "    base_dir = \"data/ravdess-emotional-speech-audio/versions/1\"\n",
    "    output_dir = \"speech\"\n",
    "\n",
    "    for actor in os.listdir(base_dir):\n",
    "        actor_path = os.path.join(base_dir, actor)\n",
    "        if os.path.isdir(actor_path) and actor.startswith(\"Actor_\"):\n",
    "            actor_num = int(actor.split(\"_\")[1])\n",
    "            if actor_num > 22: # change actor number here because it keeps crashing midway\n",
    "                print(f\"Processing {actor}...\")\n",
    "                for file in os.listdir(actor_path):\n",
    "                    if file.endswith(\".wav\"):\n",
    "                        SAMPLE_SPEECH = os.path.join(actor_path, file)\n",
    "                        SPEECH_WAVEFORM, SAMPLE_RATE = torchaudio.load(SAMPLE_SPEECH)\n",
    "\n",
    "                        # Define transform\n",
    "                        spectrogram = T.Spectrogram(n_fft=512)\n",
    "\n",
    "                        # Perform transform\n",
    "                        spec = spectrogram(SPEECH_WAVEFORM)\n",
    "                        \n",
    "                        fig, ax = plt.subplots()\n",
    "                        plot_waveform(SPEECH_WAVEFORM, SAMPLE_RATE, title=None, ax=ax)\n",
    "                        waveform_path = os.path.join(output_dir, f\"{actor}_{file}_waveform.png\")\n",
    "                        plt.savefig(waveform_path)\n",
    "                        plt.close(fig)\n",
    "\n",
    "                        # Create figure for spectrogram\n",
    "                        fig, ax = plt.subplots()\n",
    "                        plot_spectrogram(spec[0], title=None, ax=ax)\n",
    "                        spectrogram_path = os.path.join(output_dir, f\"{actor}_{file}_spectrogram.png\")\n",
    "                        plt.savefig(spectrogram_path)\n",
    "                        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting speech waveforms and spectogram to pd dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     emotion                          file_path  \\\n",
      "0    disgust  Actor_24_03-01-07-01-02-01-24.wav   \n",
      "1  surprised  Actor_24_03-01-08-02-01-01-24.wav   \n",
      "2        sad  Actor_24_03-01-04-01-02-01-24.wav   \n",
      "3      angry  Actor_24_03-01-05-01-01-01-24.wav   \n",
      "4        sad  Actor_24_03-01-04-02-01-01-24.wav   \n",
      "\n",
      "                                  spectrogram_tensor  \n",
      "0  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
      "1  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
      "2  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
      "3  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
      "4  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define base path for spectrogram images\n",
    "image_dir = \"speech\"  # Directory where spectrogram images are stored\n",
    "\n",
    "# Define transformations (convert images to tensors)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalization (optional)\n",
    "])\n",
    "\n",
    "# Function to load spectrogram as tensor\n",
    "def load_spectrogram_tensor(file_path):\n",
    "    filename = os.path.basename(file_path)\n",
    "    spectrogram_img_path = os.path.join(image_dir, f\"{filename}_spectrogram.png\")  # Construct spectrogram path\n",
    "    \n",
    "    # Load image if it exists, else return None\n",
    "    if os.path.exists(spectrogram_img_path):\n",
    "        image = Image.open(spectrogram_img_path).convert(\"L\")  # Convert to grayscale\n",
    "        return transform(image)  # Convert to tensor\n",
    "    return None  # If file doesn't exist, return None\n",
    "\n",
    "# Apply function to extract spectrogram tensors\n",
    "df[\"spectrogram_tensor\"] = df[\"file_path\"].apply(load_spectrogram_tensor)\n",
    "\n",
    "# Display updated DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     emotion                          file_path  \\\n",
      "0    disgust  Actor_24_03-01-07-01-02-01-24.wav   \n",
      "1  surprised  Actor_24_03-01-08-02-01-01-24.wav   \n",
      "2        sad  Actor_24_03-01-04-01-02-01-24.wav   \n",
      "3      angry  Actor_24_03-01-05-01-01-01-24.wav   \n",
      "4        sad  Actor_24_03-01-04-02-01-01-24.wav   \n",
      "\n",
      "                                  spectrogram_tensor  \\\n",
      "0  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
      "1  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
      "2  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
      "3  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
      "4  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...   \n",
      "\n",
      "                                     waveform_tensor  \n",
      "0  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
      "1  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
      "2  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
      "3  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n",
      "4  [[[tensor(1.), tensor(1.), tensor(1.), tensor(...  \n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define base path for waveform images\n",
    "image_dir = \"speech\"  # Directory where waveform images are stored\n",
    "\n",
    "# Define transformations (convert images to tensors)\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),    # Random horizontal flip\n",
    "    transforms.RandomRotation(30),        # Random rotation of up to 30 degrees\n",
    "    transforms.RandomResizedCrop(224),    \n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalization (optional)\n",
    "])\n",
    "\n",
    "# Function to load waveform as tensor\n",
    "def load_waveform_tensor(file_path):\n",
    "    filename = os.path.basename(file_path)  # Extract filename (e.g., \"Actor_01_03-01-01-01-01-01-01.wav\")\n",
    "    waveform_img_path = os.path.join(image_dir, f\"{filename}_waveform.png\")  # Construct waveform path\n",
    "    \n",
    "    # Load image if it exists, else return None\n",
    "    if os.path.exists(waveform_img_path):\n",
    "        image = Image.open(waveform_img_path).convert(\"L\")  # Convert to grayscale\n",
    "        return transform(image)  # Convert to tensor\n",
    "    return None  # If file doesn't exist, return None\n",
    "\n",
    "# Apply function to extract waveform tensors\n",
    "df[\"waveform_tensor\"] = df[\"file_path\"].apply(load_waveform_tensor)\n",
    "\n",
    "# Display updated DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "disgust      192\n",
       "surprised    192\n",
       "sad          192\n",
       "angry        192\n",
       "fearful      192\n",
       "happy        192\n",
       "neutral       96\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 480, 640])\n",
      "torch.Size([1, 480, 640])\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0, 2].shape)\n",
    "print(df.iloc[0, 3].shape)\n",
    "\n",
    "#combined = torch.cat([tensor1, tensor2], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['emotion'], random_state=42)\n",
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.iloc[0]['spectrogram_tensor']\n",
    "\n",
    "emotion_to_num = {\n",
    "    \"neutral\": 0,\n",
    "    \"happy\": 1,\n",
    "    \"sad\": 2,\n",
    "    \"angry\": 3,\n",
    "    \"fearful\": 4,\n",
    "    \"disgust\": 5,\n",
    "    \"surprised\": 6\n",
    "}\n",
    "\n",
    "train_set = []\n",
    "test_set = []\n",
    "\n",
    "#create train set \n",
    "for i in range(len(train_df)):\n",
    "  #concat the spectogram and waveform tensors into one \n",
    "  combined = torch.cat([train_df.iloc[i]['spectrogram_tensor'], train_df.iloc[i]['waveform_tensor']], dim=0)\n",
    "  label_tensor = torch.tensor(emotion_to_num[train_df.iloc[i]['emotion']], dtype=torch.long)\n",
    "  #save to dataset \n",
    "  train_set.append((combined, label_tensor))\n",
    "\n",
    "#create test set \n",
    "for i in range(len(test_df)):\n",
    "  #concat the spectogram and waveform tensors into one \n",
    "  combined = torch.cat([test_df.iloc[i]['spectrogram_tensor'], test_df.iloc[i]['waveform_tensor']], dim=0)\n",
    "  label_tensor = torch.tensor(emotion_to_num[test_df.iloc[i]['emotion']], dtype=torch.long)\n",
    "  test_set.append((combined, label_tensor))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8  # <-- Please change this as necessary\n",
    "NUM_WORKERS = 4  # <-- Use more workers for more CPU threads\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=2, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 120 * 160, 7)  # Fixed input size\n",
    "        self.fc2 = nn.Linear(128, 7)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Conv + ReLU + Pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Conv + ReLU + Pooling\n",
    "        x = torch.flatten(x, start_dim=1)  # Flatten for the FC layer\n",
    "        #x = F.relu(self.fc1(x))  # Hidden layer\n",
    "        x = self.dropout(x)  # Apply dropout after the fully connected layer\n",
    "        x = self.fc1(x)  # Output logits (no softmax needed before CrossEntropyLoss)\n",
    "        return x\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(96, kernel_size=11, stride=4, padding=1),\n",
    "            nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.LazyConv2d(256, kernel_size=5, padding=2), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.LazyConv2d(256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(),\n",
    "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(p=0.5),\n",
    "            nn.LazyLinear(4096), nn.ReLU(),nn.Dropout(p=0.5),\n",
    "            nn.LazyLinear(7))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "\n",
    "def train(train_loader, model, optimizer, criterion, n_epochs=10, patience=3):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    lastepoch_loss = float('inf')  # Initialize to a very high value\n",
    "    patience_counter = 0  # Count epochs without improvement\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        batch_losses = []\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for j, data in enumerate(train_loader, 0):  \n",
    "            x_batch, y_batch = data\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)  \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_predictions = model(x_batch)\n",
    "            loss = criterion(y_predictions, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "            _, predicted = torch.max(y_predictions, 1)  \n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "        epoch_loss = np.mean(batch_losses)\n",
    "        losses.append(epoch_loss)\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "\n",
    "        print(f\"Train Loss: {epoch_loss:.4f} | Train Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if abs(lastepoch_loss - epoch_loss) < 0.001:\n",
    "            patience_counter += 1\n",
    "        else:\n",
    "            patience_counter = 0  # Reset if loss improves\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Stopping early after {epoch+1} epochs due to lack of improvement.\")\n",
    "            break\n",
    "\n",
    "        lastepoch_loss = epoch_loss  # Update for next epoch\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "mymodel = AlexNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mymodel.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:20<17:01, 20.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1075 | Train Accuracy: 14.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:41<16:39, 20.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9367 | Train Accuracy: 14.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [01:02<16:18, 20.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9305 | Train Accuracy: 14.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [01:23<15:58, 20.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9288 | Train Accuracy: 15.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [01:44<15:37, 20.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9304 | Train Accuracy: 12.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [02:04<15:16, 20.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9294 | Train Accuracy: 15.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [02:25<14:55, 20.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9288 | Train Accuracy: 13.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [02:46<14:35, 20.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9293 | Train Accuracy: 14.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [03:07<14:15, 20.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9310 | Train Accuracy: 15.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [03:28<13:54, 20.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9278 | Train Accuracy: 14.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [03:49<13:34, 20.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9300 | Train Accuracy: 15.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [04:10<13:13, 20.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9279 | Train Accuracy: 14.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [04:31<12:52, 20.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9299 | Train Accuracy: 14.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [04:51<12:31, 20.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9293 | Train Accuracy: 13.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [05:12<12:09, 20.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9292 | Train Accuracy: 13.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [05:33<12:58, 22.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9291 | Train Accuracy: 13.63%\n",
      "Stopping early after 16 epochs due to lack of improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "losses = train(train_loader, mymodel, optimizer, criterion, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7c0c7374ca60>]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "\n",
    "plt.plot(np.arange(len(losses)), losses, label='Loss Plot', color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # We don't need gradients for evaluation\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)  # Get model predictions\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the class with the highest score\n",
    "            total += labels.size(0)  # Increment the total number of samples\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    accuracy = 100 * correct / total  # Accuracy as a percentage\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 28.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = calculate_accuracy(mymodel, test_loader)\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
